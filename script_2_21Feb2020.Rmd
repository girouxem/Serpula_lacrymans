---
title: "Serpula_lacymans"
author: "Emily Giroux"
date: "1/29/2020"
output: pdf_document
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
header-includes: \usepackage{xcolor}
---

```{r, global_options, eval=TRUE, echo=FALSE, cache=TRUE}
#Set the global options for knitr
library("knitr")
opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE, fig.align = 'center',cache = FALSE, 
               collapse = TRUE, echo = FALSE, eval = FALSE, include = FALSE, message = FALSE, 
               quietly = TRUE, results = 'hide', warn.conflicts = FALSE, warning = FALSE)
```

```{r, installation1, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#Installing required packages
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

if(!require(devtools)) install.packages("devtools")

if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

library("BiocManager")
.cran_packages <- c("data.table", "doSNOW", "dplyr", "filesstrings", "ggplot2", 
                    "gridExtra", "kableExtra", "knitr", "mgsub", "reshape2", "rprojroot",    
                    "R.utils", "seqinr", "stringr", "textutils", "tidyr")
.bioc_packages <- c("BiocStyle", "Biostrings", "SRAdb")
.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  BiocManager::install(.bioc_packages[!.inst], ask = FALSE)
}
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)

devtools::install_github("ropensci/rentrez")
```

Source our custom R scripts:    
For this we will use the rprojroot package to set the directory structures. This will help us when finding our files to source functions. We specify ours is an RStudio project. The root object contains a function that will help us locate our package R files regarless of our current working directory.
```{r sourcing_my_functions, echo=FALSE, eval=TRUE, include=FALSE, cache=TRUE}
library("rprojroot")
root <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts  <- dir(root$find_file("R", path = root$find_file()))
scriptsl <- paste(scriptsPath, scripts, sep = "//")
```

Record the path to the environment images directory:
```{r}
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory"
lapply(scriptsl, source)
```

Make a dedicated analysis directory for this project in my PIRL_working_directory
```{r}
analysis <- "serpulaLacrymans"
sharedPathAn <- paste(sharedPath, analysis, sep = "/")
dir.create(sharedPathAn, showWarnings = TRUE, recursive = FALSE)
```

Make a dedicated environment directory for this project in my GitHub_Repos/r_environments directory:
```{r}
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/Serpula_lacrymans/"
dir.create("/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/Serpula_lacrymans", 
           showWarnings = TRUE, recursive = FALSE)
```

Create the name for the environment data file for this project
```{r}
baseImage <- "Serpula_lacrymans_polished_Aug2020.RData"
load(paste(imageDirPath, baseImage, sep = ""))
save.image(paste(imageDirPath, baseImage, sep = ""))
```

When re-restart a session, we can load out environment quickly by running this chunk:
```{r}
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory"
analysis <- "serpulaLacrymans"
sharedPathAn <- paste(sharedPath, analysis, sep = "/")
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/Serpula_lacrymans/"
baseImage <- "Serpula_lacrymans_21_Feb2020.RData"
load(paste(imageDirPath, baseImage, sep = ""))
```

Save a file of the S. lacrymans ITS sequence that we have as our own reference file in the sharedPathAn directory, or paste it here and load it with the Biostrings package:
```{r}
library("Biostrings")
slacrymansITSseq <- "AAGGATCATTATCGATTCAACGAAGTGCTTGTGAAGTTGTGCTGGCCTCTCTCGAGGCATGTGCACGCTTTATGGGTCTTCACACACCCACACCTGTGAACCAACTGTAGGTCTTTCGGGACCTATGTCTTCTCATAACACACTGTATGTCTAGAATGTCATTATGATTATCGTCTGCTTCCTCTGTGGAGTTGGGCTGATAAGATAAACAATATACAACTTTCAGCAACGGATCTCTTGGCTCTCGCATCGATGAAGAACGCAGCGAATTGCGATATGTAATGTGAATTGCAGATTTTCAGTGAATCATCGAATCTTTGAACGCACCTTGCGCTCCTTGGTATTCCGAGGAGCATGCCTGTTTGAGTGTCATTAAATTCTCAACCCCATCAATTTGTTTTGATTGTGGGCTTGGATTGTGGGGGCTTGCTGGTGGACTCTTGTTCATCGGCTCCTCTGAAATGTATTAGCAAAGGTTGATGTGCGAACCAGTGTCTCGGTGTGATAATGATCATCGTGTCTGACGTGCAGTGTTCCTGTGCTTACAGTCGTTGTCGCAAGACAACATTTTTGAACCTTTGACCTCAAATCAGGTAGGATTACCCGCTGAACTTAAG"
```

Create a fasta entry with the name for the sequence, and write it to file:
```{r}
slacrymansITSobj <- Biostrings::DNAStringSet(slacrymansITSseq)
names(slacrymansITSobj) <- "slacrymansITS"
Biostrings::writeXStringSet(slacrymansITSobj, paste(sharedPathAn, "slacrymansITS_nuccoreGU066830.1.fasta", sep = "/"))
```

If we want to read the ITS fastq from file into R:
```{r}
slacrymansITS <- Biostrings::readDNAStringSet(paste(sharedPathAn, "slacrymansITS_nuccoreGU066830.1.fasta", sep = "/"), format = "fasta")
```

NCBI blast to design in-silico S. lacrymans probes
1. Blastn ITS sequence with parameters:
- Database: Nucleotide collection (nr/nt)
- Organism: Exclude Serpula lacrymans (taxid:85982)
- Exclude: uncultured/environmental sample sequences
2. With NCBI BLAST results:
- Download FASTA (aligned sequences)
- Import into Geneious
3. With query and blast sequences in Geneious:
- Select all, including query, and run MAFFT nucleotide alignment 
- E-INS-i, with default, and 
- Automatically determine sequences' direction
- adjust direction more accurately
4. Design 80 bp in-silico probes specific to query S. lacrymans:
In alignment,
- set query as the reference sequence
- identify 3 regions of hypervariability to reference sequence to act as in-silico probes, 80 bp length each
- write probes to file add here and register saved file

In-silico probes designed in Geneious
```{r}
ncbiBlastResFa <- paste(sharedPathAn, "blast_slacrymansITS_resultsAligned.fasta", sep = "/")
geneiousFaAlign <- paste(sharedPathAn, "blast_slacrymansITS_resultsAlignedMAFFT.fasta", sep = "/")
slacrymansProbesITS   <- paste(sharedPathAn, "slacrymansITSinSilicoProbesITS.fasta", sep = "/")
probesSeqs <- Biostrings::readDNAStringSet(slacrymansProbesITS, format = "fasta")
slacrymans550_656_ITS <- "CATCGTGTCTGACGTGCAGTGTTCCTGTGCTTACAGTCGTTGTCGCAAGACAACATTTTTGAACCTTTGACCTCAAATCA"
slacrymans403_494_ITS <- "AACCCCATCAATTTGTTTTGATTGTGGGCTTGGATTGTGGGGGCTTGCTGGTGGACTCTTGTTCATCGGCTCCTCTGAAA"
slacrymans136_220_ITS <- "GTCTTCTCATAACACACTGTATGTCTAGAATGTCATTATGATTATCGTCTGCTTCCTCTGTGGAGTTGGGCTGATAAGAT"

slacrymansITSprobes <- Biostrings::DNAStringSet(c(slacrymans550_656_ITS, slacrymans403_494_ITS, slacrymans136_220_ITS))
names(slacrymansITSprobes) <- c("slacrymans550_656_ITS", "slacrymans403_494_ITS", "slacrymans136_220_ITS")
```


Obtain the SRAmetadb - it's a big file and takes a long time, only need to do once unless it's been a long time since first download and there has been an update to the database. 
```{r}
library("SRAdb")
dbDirPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/Databases/"
dir.create(paste(dbDirPath, "sraDB", sep = "/"), showWarnings = TRUE, recursive = FALSE)
srafile <- SRAdb::getSRAdbFile(destdir = paste(dbDirPath, "sraDB", sep = ""), destfile = "SRAmetadb.sqlite.gz", method="auto")
```

Create the directory where we will bring in the SRA files:
```{r}
sra_searchDirEBI <- paste(sharedPathAn, "sra_searchEBI", sep = "/")
dir.create(sra_searchDirEBI, showWarnings = TRUE, recursive = FALSE)
```

Use SQLite to connect to the SRA database and get SRAs from EBI: These are those SRAs that were not on NCBI. Obtained these as their fastq.
```{r}
library("data.table")
library("SRAdb")
timeStart <- proc.time()
proc.time() - timeStart
file.info(srafile)
sra_con <- RSQLite::dbConnect(dbDriver("SQLite"), srafile)

## Fulltext search SRA meta data using SQLite fts3 module
rs <- as.data.table(SRAdb::getSRA(search_terms = 'forest* decay ITS*', out_types=c('run','study'), sra_con=sra_con))
rs2 <- as.data.table(SRAdb::getSRA(search_terms = 'ITS Quebec soil*', out_types=c('run','study'), sra_con=sra_con))
rs3 <- as.data.table(SRAdb::getSRA(search_terms = 'ITS Quebec forest* soil', out_types=c('run','study'), sra_con=sra_con))
rs4 <- as.data.table(SRAdb::getSRA(search_terms = 'ITS Ontario indoor', out_types=c('run','study'), sra_con=sra_con))
rs5 <- as.data.table(SRAdb::getSRA(search_terms = 'indoor fungal', out_types=c('run','study'), sra_con=sra_con))
rs6 <- as.data.table(SRAdb::getSRA(search_terms = 'indoor fungal decay', out_types=c('run','study'), sra_con=sra_con))
rs7 <- as.data.table(SRAdb::getSRA(search_terms = 'building fungal decay', out_types=c('run','study'), sra_con=sra_con))
rs8 <- as.data.table(SRAdb::getSRA(search_terms = 'fungal decay', out_types=c('run','study'), sra_con=sra_con))

rsFull <- rbind(rs, rs2, rs3, rs4, rs5, rs6, rs7, rs8)
rm(rs, rs2, rs3, rs4, rs5, rs6, rs7, rs8)

# Terminate our SRA connection
RSQLite::dbDisconnect(sra_con)
```

Look at the SRA results and remove irrelevant entries:
```{r}
length(unique(rsFull$run))
rsUnique <- unique(rsFull, by = "run")
excludeKeyWords <- c("China|Chinese|Idaho|tropical|Bolivian|agricul*|LUKAS|Gut|Spacecraft|space station|cadaver*|dental|kitchen sink*|*rabidopsis|oral|Oral|RNA*|*ranscriptom*|*exual")
rsRemoved1 <- rsUnique[!(study_title %like% excludeKeyWords)]

rsRemoved1ExtraMeta <- sraConvert(in_acc = rsRemoved1$run, out_type = c("sra", "submission", "study", "sample", "experiment", "run"), sra_con = sra_con)
rsRemoved1ExtraMeta <- as.data.table(rsRemoved1ExtraMeta)
```

Here are the metadata elements we want to retrieve for each SRA run. We create empty columns that we will fill in in the next loop:
```{r}
library("data.table")
xx <- c("title", "uid", "accession", "publicationdate", "organisation", "taxonomy", "organism", "sourcesample", 
        "isolationSource", "sampleName", "host", "collectionDate", "geographicLocation", "geoCoordinates", "ownerFullName")
for(i in xx)rsRemoved1ExtraMeta[,i] <- NA
```

Set up our access to rentrez so that we can collect the metadata we want for our SRA runs. Note that I had to register to obtain the API key, and then copied it to the code over here. Each individual needs to do this themselves as the API key is associated to a user's account.
```{r}
library("rentrez")
apiKey <- "c7e1615b346269c43325b11d5823b4169a07"
set_entrez_key(apiKey)
entrez_db_searchable(db = "biosample")
```

Collect metadata for each SRA run, via rentrez, in a loop. Note, the following are custom functions saved to the R folder and loaded with this script:    
extractIsolationSource     
extractSampleName    
extractCollectionDate    
extractGeographicLocation     
extractGeoCoordinates     
```{r}
library("XML")
library("stringr")
library("rentrez")

# Use the dt table to build upon the metadata throughout the loop:
dt <- rsRemoved1ExtraMeta

for(i in 1:nrow(dt)){
  rentrez::set_entrez_key(apiKey)
  res <- entrez_search("biosample", term=dt$sample[i])
  esums <- entrez_summary("biosample", id = res$ids)
  title <- extract_from_esummary(esums, "title")
  uid <- extract_from_esummary(esums, "uid")
  accession <- extract_from_esummary(esums, "accession")
  publicationdate <- extract_from_esummary(esums, "publicationdate")
  organisation <- extract_from_esummary(esums, "organization")
  taxonomy <- extract_from_esummary(esums, "taxonomy")
  organism <- extract_from_esummary(esums, "organism")
  sourcesample <- extract_from_esummary(esums, "sourcesample")
  dt$title[i] <- title
  dt$uid[i] <- uid
  dt$accession[i] <- accession
  dt$publicationdate[i] <- publicationdate
  dt$organisation[i] <- organisation
  dt$taxonomy[i] <- taxonomy
  dt$organism[i] <- organism
  dt$sourcesample[i] <- sourcesample
  sampledata <- extract_from_esummary(esums, "sampledata")
  raw_html <- textutils::HTMLdecode(sampledata)
  parsed_html <- XML::htmlTreeParse(raw_html, useInternalNodes = TRUE) 
  dt$isolationSource[i] <- extractIsolationSource(parsed_html)
  dt$sampleName[i] <- extractSampleName(parsed_html)
  dt$collectionDate[i] <- extractCollectionDate(parsed_html)
  dt$geographicLocation[i] <- extractGeographicLocation(parsed_html)
  dt$geoCoordinates[i] <- extractGeoCoordinates(parsed_html)
  print(dt[i])
  Sys.sleep(0.54) # Set a delay for api restriction issues, unless you set an api key
}
```

Let's see what we have collected from our biosample metadata. Based on the keywords we find in the geographic location, we can exclude many of the non-relevant SRAs:
```{r}
library("data.table")
unique(dt$organisation)
unique(dt$geographicLocation)

filteredDT <- dt 
table(filteredDT$geographicLocation)

excludeKeyWords <- c("California|China|Finland|Germany|Hawaii|Hong Kong|Korea|Malaysia|Norway|Poland|Russia|San Francisco|Singapore|Slovenia|United Kingdom")
filteredDT <- filteredDT[!(geographicLocation %like% excludeKeyWords)]
table(filteredDT$geographicLocation)
unique(filteredDT$isolationSource)
table(filteredDT$organism)
```
From a previous run of this pipeline, I know that there are no fastq files associated with the following SRAs and so I remove them now from our set:    
ERR1864558    
ERR1864569      
ERR1864590     
ERR1864595    
     
Also, I previously noticed that there are some very large SRA files that are not worth getting since they are not really relevant to what we want. In the interest of computational resources, I'm removing them. I also am removing all those with organisation details as "Laboratory of Fungal Genomics and Evolution, Institute of Biochemistry, Synthetic and Systems Biology Unit, Biological Research Centre, Hungarian Academy of Sciences".          
SRR3500549  Sample from Fibularhizoctonia sp. CBS 109695
SRR3928183  Generic sample from Jaapia argillacea MUCL 33604    
SRR1144937  California Institute of Technology, Jet Propulsion Laboratory     
     
Also, the following SRAs have sequence errors, and are also not relevant (learned this the hard way downstream):     
SRR7813209    All of these from University of Minnesota, Postia placenta
SRR7813210    
SRR7813212    
SRR7813215    
SRR7813216    
SRR7813217    
SRR7813218    
SRR7813220    
SRR7813221    
SRR7813223    
SRR7813224
SRR7813225    
SRR7813226    
SRR7813228    
SRR7813231    
```{r}
library("data.table")
excludeSRAs <- c("ERR1864558|ERR1864569|ERR1864590|ERR1864595|SRR3500549|SRR3928183|SRR1144937")
filteredDT <- filteredDT[!(run %like% excludeSRAs)]
table(filteredDT$run)

# Exclude those with organism "Postia placenta":
excludeOrganisms <- "Postia placenta"
filteredDT <- filteredDT[!(organism %like% excludeOrganisms)]

table(filteredDT$organisation)
excludeOrganisations <- c("Laboratory of Fungal Genomics and Evolution, Institute of Biochemistry, Synthetic and Systems Biology Unit, Biological Research Centre, Hungarian Academy of Sciences|Biodiversity and Biotechnology of Fungi - BBF, INRA - Aix Marseille University|UC BERKELEY|UC Berkeley|UW/FPL|European Bioinformatics Institute")
filteredDT <- filteredDT[!(organisation %like% excludeOrganisations)]
```

Print a list of the SRAs remaining:
```{r}
library("data.table")
write.table(filteredDT$run, file = paste(sharedPathAn, "srasList.txt", sep = "/"), row.names = FALSE, col.names = FALSE, quote = FALSE)
```

**** Note - I downloaded all the SRAs from the list of filtered SRAs written in the previous chunk. Then I continued on with the script and further eliminated problematic files, trimming the SRA list even further. Here I'm just capturing the files that remained and updating my set. Because some of the fastq files have problems, I need to re-download them anew to see if that fixes things. 
```{r}
# Get an updated metadata table for the sra fastqs we successfully obtained, and record their file names and paths:
library("data.table")
sraFqs <- data.table(basename=list.files(sra_searchDirEBI))
sraFqs$run <- gsub(".fastq.gz|.fastq", "", sraFqs$basename)
sraFqs$run <- gsub("_1|_2", "", sraFqs$run)
fqgzList <- sraFqs[which(sraFqs$run %in% filteredDT$run)]

setkey(fqgzList, run)
head(fqgzList)
setkey(filteredDT, run)
head(filteredDT)

fqDT <- filteredDT[fqgzList]
fqDT$fqPath <- paste(sra_searchDirEBI, fqDT$basename, sep = "/")

# create a new list of SRAs to download:
library("data.table")
write.table(fqDT$run, file = paste(sharedPathAn, "srasListFurtherFiltered.txt", sep = "/"), row.names = FALSE, col.names = FALSE, quote = FALSE)

# print the metadata table "fqDT" to file:
tblName <- "SRA_metadata_from_fqDT"

fwrite(fqDT, file = paste(sharedPathAn, "/", tblName, ".csv", sep = ""), 
       append = FALSE, sep = ",", row.names = FALSE, col.names = TRUE, quote = FALSE)
```

The very slow way to download the fastq for the sras, then see what we have after....:
```{r}
packages <- c("data.table", "SRAdb")
lapply(packages, library, character.only = TRUE)

timeStart <- proc.time()
proc.time() - timeStart
file.info(srafile)
sra_con <- dbConnect(dbDriver("SQLite"), srafile)

getSRAfile(in_acc = fqDT$run, sra_con = sra_con, destDir = sra_searchDirEBI, 
           fileType = 'fastq', srcType = 'ftp', makeDirectory = FALSE, method = 'curl', ascpCMD = NULL)

# See which SRAs were downloaded and then see which are still missing, if any. Then repeat to fetch the remaining SRAs:
sraFqs <- list.files(sra_searchDirEBI)
srasMissing <- fqDT[which(!(fqDT$basename %in% sraFqs))]

timeStart <- proc.time()
proc.time() - timeStart
file.info(srafile)
sra_con <- dbConnect(dbDriver("SQLite"), srafile)

getSRAfile(in_acc = srasMissing$run, sra_con = sra_con, destDir = sra_searchDirEBI, 
           fileType = 'fastq', srcType = 'ftp', makeDirectory = FALSE, method = 'curl', ascpCMD = NULL)

dbDisconnect(sra_con)
```

Remove all the reverse *_2.fastq.gz read files, not needed, saves space. I did this quickly on the command line.

Get an updated metadata table for the sra fastqs we successfully obtained, and record their file names and paths:
```{r}
library("data.table")
sraFqs <- data.table(basename=list.files(sra_searchDirEBI))
sraFqs$run <- gsub(".fastq.gz|.fastq", "", sraFqs$basename)
sraFqs$run <- gsub("_1|_2", "", sraFqs$run)
fqgzList <- sraFqs[which(sraFqs$run %in% fqDT$run)]

# See which SRAs were downloaded and then see which are still missing, if any. Then repeat to fetch the remaining SRAs:
sraFqs <- data.table(basename=list.files(sra_searchDirEBI))
sras <- fqDT[which(fqDT$basename %in% sraFqs$basename)]
fqDT2 <- sras
```

### BLAST method     
For going the blast route, uncompress the merged.fq.fa file. Then use seqtk to convert it to fasta format:     

### Full set:
For the Full fastq gz file, there is a problem making it crash when unzipping - probably too large a file. Will try:
- copy individual fastq.gz files to fullSRA directory
- gunzip the fastq.gz files
- run fq2fa on individual fastq files
- merge individual fasta files
```{r}
prefix <- "cp_gunzip_FqSRAFull"
cmd <- paste("cd ", sharedPathAn, " && cp ", fqDT2$fqPath,  " ", fullSRA, "/", fqDT2$basename, 
             " && cd ", fullSRA, " && gunzip ", fullSRA, "/", fqDT2$basename, sep = "") 
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
fqDT2$basenameFqUngz <- gsub(".gz", "", fqDT2$basename)
fqDT2$basenameFaUngz <- gsub(".fastq", ".fa", fqDT2$basenameFqUngz)
```
**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

# Look at output files, to see if any caused problems - these files should be removed so as not to break the pipeline. All good files should be 0 size. This will sort logs by size, and the ones at the bottom are files that are larger and indicate an error or problem occurred. 

$ ls -Srl cp_gunzip_FqSRAFull/*.sub.e*
-rw-r--r-- 1 CFIA-ACIA+girouxeml CFIA-ACIA+domain users 147 Jul  8 11:52 cp_gunzip_FqSRAFull/cp_gunzip_FqSRAFull2024.sub.e922938
-rw-r--r-- 1 CFIA-ACIA+girouxeml CFIA-ACIA+domain users 162 Jul  8 11:54 cp_gunzip_FqSRAFull/cp_gunzip_FqSRAFull2026.sub.e922940
-rw-r--r-- 1 CFIA-ACIA+girouxeml CFIA-ACIA+domain users 164 Jul  8 11:54 cp_gunzip_FqSRAFull/cp_gunzip_FqSRAFull2029.sub.e922943

# These subs correspond to the following SRAs (use cat on these subs to see the run IDs):
cp_gunzip_FqSRAFull2024.sub.e922938 = cp_gunzip_FqSRAFull2024.sub = run SRR3500546
cp_gunzip_FqSRAFull2026.sub.e922940 = cp_gunzip_FqSRAFull2026.sub = run SRR3500548
cp_gunzip_FqSRAFull2029.sub.e922943 = cp_gunzip_FqSRAFull2029.sub = run SRR3928184
```{r}
# Check the data for these in the fqDT2 table:
library("data.table")
setkey(fqDT2, "run")
bad <- fqDT2[.(c("SRR3500546", "SRR3500548", "SRR3928184"))]

# Remove these from the fqDT2 set:
fqDTdropBad <- fqDT2[!.(c("SRR3500546", "SRR3500548", "SRR3928184"))]

# Delete the fasta files for these from the fullSRA directory, since they failed, they will present in their .gz format:
unlink(paste(fullSRA, bad$basename, sep = "/")) # Deletes bad files
file.exists(paste(fullSRA, bad$basename, sep = "/")) # checks if files are actually gone

fqDT2 <- fqDTdropBad
rm(fqDTdropBad)
```

# Convert fastq to fa format
```{r}
prefix <- "fq2faFullSRASet"
cmd <- paste("conda activate insilicoPrimer && cd ", fullSRA,
             " && seqtk seq -a ", paste(fullSRA, fqDT2$basenameFqUngz, sep = "/"), 
             " > ", paste(fullSRA, fqDTdropBad$basenameFaUngz, sep = "/"), 
             " && conda deactivate", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
# Don't forget to make sure no errors occured by checking for log files with greater than 0 size, as in previous step
```
**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Take the opportunity to filter the reads, remove all those entries with min length less than 25, also remove those by entropy to discard reads that are all TTTT or AAAAs before merging the files into one big fa. bbduk may be able to accomplish this quickly.     
Note - it may have been better to do this on the fastq files, since bbduk uses quality info during processing.
```{r}
prefix <- "processSRAFasta"
cmd <- paste("conda activate insilicoPrimer && cd ", fullSRA,
             " && bbduk.sh in=", paste(fullSRA, fqDT2$basenameFaUngz, sep = "/"),
             " out=", paste(fullSRA, paste("clean_", fqDT2$basenameFaUngz, sep = ""), sep = "/"),
             " minlen=25 maxns=-1 tossjunk=t trimpolya=15 interleaved=f", 
             " && conda deactivate ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
# cat of all good fa files:
```{r}
prefix <- "catFullFaClean"
cmd <- paste("cd ", fullSRA, " && cat clean_*.fa > merged.fa ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
faFullPath <- paste(fullSRA, "merged.fa", sep = "/")
```

# Clean up temporary files:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

*** Note:    
On the command line I removed all the characters after the first space in the fasta header using awk:    
$ awk '/^>/ {$0=$1} 1' merged.fa > cleaned.merged.fa
    

### Make a blast database of the full merged fa file:
```{r}
faCleanFullPath <- paste(fullSRA, "cleaned.merged.fa", sep = "/")
blastDB <- paste(sharedPathAn, "blastdb", sep = "/")
dir.create(blastDB, showWarnings = TRUE, recursive = FALSE)

prefix <- "makeblastDB_cleanfull"
cmd <- paste("conda activate insilicoPrimer && cd ", blastDB, 
             " && makeblastdb -in ", faCleanFullPath,
             " -dbtype nucl -out sraCleanFull -logfile sraCleanFull.log && conda deactivate", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

After completion of "makeblastDB", compress all the fasta and fq files to conserve disc space:
```{r}
prefix <- "gzipAllseqFilesInFullSRA"
cmd <- paste("cd ", fullSRA, " && gzip * ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Split the blast database volumes (total 236) into 3 separate directories, so that there are no more than 100 volumes per directory. This will prevent an error in downstream blast runs where the full blast database ends up having a negative total number of sequences in the summary and returns zero hits when running blast on it.    
```{r}
subdirs <- c("blastdbTo100_volSubset", "blastdbFrom101_volSubset", "blastdbFrom201_volSubset")
for (j in 1:length(subdirs)){
  folder <- dir.create(paste0(sharedPathAn, "/", subdirs[j]))
}
```
Copy the blastdb/sraCleanFull.nal, blastdb/sraCleanFull.log, blastdb/sraCleanFull.perl to each of the newly created directories.    
Open a text editor and edit the sraCleanFull.nal files in each directory to list only the volumes in the subset directories. For example, in the blastdbTo100_volSubset/sraCleanFull.nal delete all names of volumes above 100.    
     
Load up each of the blast databases:
```{r}
devtools::install_github("mhahsler/rBLAST")
library("rBLAST")

# Full blastDB paths to subset databases:
blTo100path <- paste(sharedPathAn, subdirs[1], "sraCleanFull", sep = "/")
blTo100 <- rBLAST::blast(db=blTo100path)
blTo100

blFrom101path <- paste(sharedPathAn, subdirs[2], "sraCleanFull", sep = "/")
blFrom101 <- rBLAST::blast(db=blFrom101path)
blFrom101

blFrom201path <- paste(sharedPathAn, subdirs[3], "sraCleanFull", sep = "/")
blFrom201 <- rBLAST::blast(db=blFrom201path)
blFrom201
```

Blast our in-silico probes against the custom databases we've built:    
If you get the error:     
Error: Executable for blastn not found! Please make sure that the software is correctly installed and, if necessary, path variables are set.     
You need to find where blastn is onyour system and add it to the system paths:     
```{r}
?blast     
Sys.which("blastn")     
Sys.getenv("PATH")    
# [1] "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"     
Sys.setenv(PATH=paste(Sys.getenv("PATH"), "/opt/bio/ncbi-blast+/bin", sep= .Platform$path.sep))     
Sys.getenv("PATH")
# [1] "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bio/ncbi-blast+/bin/blastn:/opt/bio/ncbi-blast+/bin"     
Sys.which("blastn")     
# "/opt/bio/ncbi-blast+/bin/blastn"      
```

Query the probes to the databases:
```{r}
custFmt <- "qaccver saccver pident length mismatch gapopen qstart qend sstart send evalue bitscore sseq"
# Volumes 00-100
library("rBLAST")
clto100_1 <- predict(blTo100, slacrymansITSprobes[1], custom_format = custFmt)
clto100_2 <- predict(blTo100, slacrymansITSprobes[2], custom_format = custFmt)
clto100_3 <- predict(blTo100, slacrymansITSprobes[3], custom_format = custFmt)

# Volumes 101-200
clfrom101_1 <- predict(blFrom101, slacrymansITSprobes[1], custom_format = custFmt)
clfrom101_2 <- predict(blFrom101, slacrymansITSprobes[2], custom_format = custFmt)
clfrom101_3 <- predict(blFrom101, slacrymansITSprobes[3], custom_format = custFmt)

# Volumes 201-236
clfrom201_1 <- predict(blFrom201, slacrymansITSprobes[1], custom_format = custFmt)
clfrom201_2 <- predict(blFrom201, slacrymansITSprobes[2], custom_format = custFmt)
clfrom201_3 <- predict(blFrom201, slacrymansITSprobes[3], custom_format = custFmt)

clAll <- rbind(clto100_1, clto100_2, clto100_3, clfrom101_1, clfrom101_2, clfrom101_3, clfrom201_1, clfrom201_2, clfrom201_3)
clAll$SRA <- gsub("\\..*", "", clAll$saccver)
table(clAll$SRA)
```


Extract the metadata from the fqDT2 (final metadata reference table) for all the SRAs that returned blast hits (clAll datatable):
```{r}
library("data.table")
hitsSRAs <- data.table(run=unique(clAll$SRA))
hitSRAfqDT2 <- fqDT2[which(fqDT2$run %in% hitsSRAs$run)]

# Write the table to file:
fwrite(hitSRAfqDT2, paste(sharedPathAn, "/metadata_of_SRAreturningBlastHits.csv", sep = ""), row.names = FALSE, col.names = TRUE, quote = FALSE, sep = ",")
```


Extract the sequences. You can then import the written fasta file and take a closer look at the sequences that returned blast hits:
```{r}
library("data.table")
library("Biostrings")
hitSRAseqdt <- clAll[, c("saccver", "sseq")]
hitSRAseqs  <- DNAStringSet(hitSRAseqdt$sseq)
names(hitSRAseqs) <- hitSRAseqdt$saccver
writeXStringSet(hitSRAseqs, paste(sharedPathAn, "sequencesOfBlastHits.fasta", sep = "/"))
```
